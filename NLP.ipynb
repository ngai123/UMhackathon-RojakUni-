{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8c9fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSentiment Analysis with TF-IDF and BERT: Fine-Tuning for different types of text sources (news / tweets / posts)\\n\\nOverview:\\nThis script demonstrates two approaches for sentiment classification of cryptocurrency-related text:\\n1. **TF-IDF with Logistic Regression**: A traditional machine learning approach using TF-IDF features to represent the text and training a logistic regression model for classification.\\n2. **BERT (DistilBERT) Fine-Tuning**: A deep learning approach that fine-tunes a pre-trained DistilBERT model for sentiment classification.\\n\\nBoth models classify the sentiment of into three categories: 'negative', 'neutral', and 'positive'.\\nNote that the first model is more geared towards a smaller training dataset. \\nHowever, the second one is more suitable for a larger dataset.\\nIdeally, we would also be using the second approach but with the (ProsusAI/finbert) model.\\n\\nWorkflow:\\n1. **TF-IDF Model**:\\n    - The dataset is loaded from a pandas DataFrame and preprocessed.\\n    - **TF-IDF Vectorization**: The text is converted into numerical features using the TF-IDF vectorizer. This vectorizer captures both unigrams and bigrams to represent the text.\\n    - **Logistic Regression**: A logistic regression model is trained using the TF-IDF features to classify the sentiments of the tweets. Sample weights are computed to handle imbalanced classes.\\n\\n2. **BERT Model (Fine-Tuning)**:\\n    - **Data Preprocessing**: The dataset is tokenized using the Hugging Face `DistilBertTokenizer`, ensuring that tweet titles are padded and truncated to a fixed length of 128 tokens.\\n    - **Label Mapping**: Sentiment labels are converted from string representations (e.g., `{'class': 'positive'}`) into integer values (2 for positive, 1 for neutral, 0 for negative).\\n    - **Model Setup**: A pre-trained **DistilBERT** model is loaded for sequence classification with 3 labels (negative, neutral, positive). The model is then fine-tuned on the sentiment classification task.\\n    - **Training**: The model is trained using the Hugging Face `Trainer` class, with evaluation metrics including accuracy and weighted F1 score. The training process uses gradient accumulation, mixed precision (FP16), and model saving after each epoch.\\n\\n3. **Evaluation**:\\n    - Both models (TF-IDF + Logistic Regression and BERT) are evaluated using the test dataset. Performance metrics such as accuracy and weighted F1 score are computed to assess how well the models predict sentiment.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sentiment Analysis with TF-IDF and BERT: Fine-Tuning for different types of text sources (news / tweets / posts)\n",
    "\n",
    "Overview:\n",
    "This script demonstrates two approaches for sentiment classification of cryptocurrency-related text:\n",
    "1. **TF-IDF with Logistic Regression**: A traditional machine learning approach using TF-IDF features to represent the text and training a logistic regression model for classification.\n",
    "2. **BERT (DistilBERT) Fine-Tuning**: A deep learning approach that fine-tunes a pre-trained DistilBERT model for sentiment classification.\n",
    "\n",
    "Both models classify the sentiment of into three categories: 'negative', 'neutral', and 'positive'.\n",
    "Note that the first model is more geared towards a smaller training dataset. \n",
    "However, the second one is more suitable for a larger dataset.\n",
    "Ideally, we would also be using the second approach but with the (ProsusAI/finbert) model.\n",
    "\n",
    "Workflow:\n",
    "1. **TF-IDF Model**:\n",
    "    - The dataset is loaded from a pandas DataFrame and preprocessed.\n",
    "    - **TF-IDF Vectorization**: The text is converted into numerical features using the TF-IDF vectorizer. This vectorizer captures both unigrams and bigrams to represent the text.\n",
    "    - **Logistic Regression**: A logistic regression model is trained using the TF-IDF features to classify the sentiments of the tweets. Sample weights are computed to handle imbalanced classes.\n",
    "\n",
    "2. **BERT Model (Fine-Tuning)**:\n",
    "    - **Data Preprocessing**: The dataset is tokenized using the Hugging Face `DistilBertTokenizer`, ensuring that tweet titles are padded and truncated to a fixed length of 128 tokens.\n",
    "    - **Label Mapping**: Sentiment labels are converted from string representations (e.g., `{'class': 'positive'}`) into integer values (2 for positive, 1 for neutral, 0 for negative).\n",
    "    - **Model Setup**: A pre-trained **DistilBERT** model is loaded for sequence classification with 3 labels (negative, neutral, positive). The model is then fine-tuned on the sentiment classification task.\n",
    "    - **Training**: The model is trained using the Hugging Face `Trainer` class, with evaluation metrics including accuracy and weighted F1 score. The training process uses gradient accumulation, mixed precision (FP16), and model saving after each epoch.\n",
    "\n",
    "3. **Evaluation**:\n",
    "    - Both models (TF-IDF + Logistic Regression and BERT) are evaluated using the test dataset. Performance metrics such as accuracy and weighted F1 score are computed to assess how well the models predict sentiment.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071ef72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Downloads\\Monash\\VS\\simple_bitcoin_prediction\\finbert_train_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing all necessary libraries for the execution of the code\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.utils import class_weight\n",
    "import ast\n",
    "import re\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, pipeline, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import torch\n",
    "import joblib\n",
    "from typing import Dict, List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c95e249f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>source</th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-05 06:52:09</td>\n",
       "      <td>{'class': 'negative', 'polarity': -0.03, 'subj...</td>\n",
       "      <td>CoinTelegraph</td>\n",
       "      <td>defi</td>\n",
       "      <td>The compensation process is expected to start ...</td>\n",
       "      <td>Allbridge to first begin repaying stuck bridge...</td>\n",
       "      <td>https://cointelegraph.com/news/allbridge-to-fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-05 06:19:00</td>\n",
       "      <td>{'class': 'neutral', 'polarity': 0.0, 'subject...</td>\n",
       "      <td>CryptoPotato</td>\n",
       "      <td>bitcoin</td>\n",
       "      <td>On-chain analytics revealed a sentiment shift ...</td>\n",
       "      <td>Bitcoin Hodl Patterns Indicate Cycle Shift to ...</td>\n",
       "      <td>https://cryptopotato.com/bitcoin-hodl-patterns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-05 05:09:44</td>\n",
       "      <td>{'class': 'negative', 'polarity': -0.04, 'subj...</td>\n",
       "      <td>CoinTelegraph</td>\n",
       "      <td>bitcoin</td>\n",
       "      <td>Ether has broken the $1,900 resistance level f...</td>\n",
       "      <td>ETH hits 7-month high ahead of Shanghai and Ca...</td>\n",
       "      <td>https://cointelegraph.com/news/eth-hits-7-mont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-05 01:09:52</td>\n",
       "      <td>{'class': 'positive', 'polarity': 0.07, 'subje...</td>\n",
       "      <td>CoinTelegraph</td>\n",
       "      <td>bitcoin</td>\n",
       "      <td>With a new quarterly production record, Marath...</td>\n",
       "      <td>Marathon Digital posts quarterly record of 2,1...</td>\n",
       "      <td>https://cointelegraph.com/news/marathon-digita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-04-04 23:49:00</td>\n",
       "      <td>{'class': 'positive', 'polarity': 0.4, 'subjec...</td>\n",
       "      <td>CryptoPotato</td>\n",
       "      <td>altcoin</td>\n",
       "      <td>The stablecoin BTG Dol will supposedly become ...</td>\n",
       "      <td>Brazilian Finance Giant BTG Pactual to Issue a...</td>\n",
       "      <td>https://cryptopotato.com/brazilian-finance-gia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18538</th>\n",
       "      <td>2021-10-27 15:17:00</td>\n",
       "      <td>{'class': 'neutral', 'polarity': 0.0, 'subject...</td>\n",
       "      <td>CryptoNews</td>\n",
       "      <td>defi</td>\n",
       "      <td>Cream Finance (CREAM) suffered another flash l...</td>\n",
       "      <td>Cream Finance Suffers Another Exploit as Attac...</td>\n",
       "      <td>https://cryptonews.com/news/cream-finance-suff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18539</th>\n",
       "      <td>2021-10-19 13:39:00</td>\n",
       "      <td>{'class': 'positive', 'polarity': 0.1, 'subjec...</td>\n",
       "      <td>CryptoNews</td>\n",
       "      <td>blockchain</td>\n",
       "      <td>Banque de France disclosed the results of its ...</td>\n",
       "      <td>French Central Bank's Blockchain Bond Trial Br...</td>\n",
       "      <td>https://cryptonews.com/news/french-central-ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18540</th>\n",
       "      <td>2021-10-18 13:58:00</td>\n",
       "      <td>{'class': 'positive', 'polarity': 0.14, 'subje...</td>\n",
       "      <td>CryptoNews</td>\n",
       "      <td>blockchain</td>\n",
       "      <td>Advancing its project to become \\x9caÂ\\xa0meta...</td>\n",
       "      <td>Facebook To Add 10,000 Jobs In EU For Metavers...</td>\n",
       "      <td>https://cryptonews.com/news/facebook-to-add-10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18541</th>\n",
       "      <td>2021-10-15 00:00:00</td>\n",
       "      <td>{'class': 'neutral', 'polarity': 0.0, 'subject...</td>\n",
       "      <td>CryptoNews</td>\n",
       "      <td>blockchain</td>\n",
       "      <td>Chinese companies are still topping the blockc...</td>\n",
       "      <td>Tech Crackdown Hasn't Halted Chinese Firms' Bl...</td>\n",
       "      <td>https://cryptonews.com/news/tech-crackdown-has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18542</th>\n",
       "      <td>2021-10-12 20:00:00</td>\n",
       "      <td>{'class': 'positive', 'polarity': 0.16, 'subje...</td>\n",
       "      <td>CryptoNews</td>\n",
       "      <td>blockchain</td>\n",
       "      <td>Within a little more than a year, Celo aims to...</td>\n",
       "      <td>Celo to Be Fastest EVM Chain by End of 2022, C...</td>\n",
       "      <td>https://cryptonews.com/news/celo-to-be-fastest...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18543 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date                                          sentiment  \\\n",
       "0      2023-04-05 06:52:09  {'class': 'negative', 'polarity': -0.03, 'subj...   \n",
       "1      2023-04-05 06:19:00  {'class': 'neutral', 'polarity': 0.0, 'subject...   \n",
       "2      2023-04-05 05:09:44  {'class': 'negative', 'polarity': -0.04, 'subj...   \n",
       "3      2023-04-05 01:09:52  {'class': 'positive', 'polarity': 0.07, 'subje...   \n",
       "4      2023-04-04 23:49:00  {'class': 'positive', 'polarity': 0.4, 'subjec...   \n",
       "...                    ...                                                ...   \n",
       "18538  2021-10-27 15:17:00  {'class': 'neutral', 'polarity': 0.0, 'subject...   \n",
       "18539  2021-10-19 13:39:00  {'class': 'positive', 'polarity': 0.1, 'subjec...   \n",
       "18540  2021-10-18 13:58:00  {'class': 'positive', 'polarity': 0.14, 'subje...   \n",
       "18541  2021-10-15 00:00:00  {'class': 'neutral', 'polarity': 0.0, 'subject...   \n",
       "18542  2021-10-12 20:00:00  {'class': 'positive', 'polarity': 0.16, 'subje...   \n",
       "\n",
       "              source     subject  \\\n",
       "0      CoinTelegraph        defi   \n",
       "1       CryptoPotato     bitcoin   \n",
       "2      CoinTelegraph     bitcoin   \n",
       "3      CoinTelegraph     bitcoin   \n",
       "4       CryptoPotato     altcoin   \n",
       "...              ...         ...   \n",
       "18538     CryptoNews        defi   \n",
       "18539     CryptoNews  blockchain   \n",
       "18540     CryptoNews  blockchain   \n",
       "18541     CryptoNews  blockchain   \n",
       "18542     CryptoNews  blockchain   \n",
       "\n",
       "                                                    text  \\\n",
       "0      The compensation process is expected to start ...   \n",
       "1      On-chain analytics revealed a sentiment shift ...   \n",
       "2      Ether has broken the $1,900 resistance level f...   \n",
       "3      With a new quarterly production record, Marath...   \n",
       "4      The stablecoin BTG Dol will supposedly become ...   \n",
       "...                                                  ...   \n",
       "18538  Cream Finance (CREAM) suffered another flash l...   \n",
       "18539  Banque de France disclosed the results of its ...   \n",
       "18540  Advancing its project to become \\x9caÂ\\xa0meta...   \n",
       "18541  Chinese companies are still topping the blockc...   \n",
       "18542  Within a little more than a year, Celo aims to...   \n",
       "\n",
       "                                                   title  \\\n",
       "0      Allbridge to first begin repaying stuck bridge...   \n",
       "1      Bitcoin Hodl Patterns Indicate Cycle Shift to ...   \n",
       "2      ETH hits 7-month high ahead of Shanghai and Ca...   \n",
       "3      Marathon Digital posts quarterly record of 2,1...   \n",
       "4      Brazilian Finance Giant BTG Pactual to Issue a...   \n",
       "...                                                  ...   \n",
       "18538  Cream Finance Suffers Another Exploit as Attac...   \n",
       "18539  French Central Bank's Blockchain Bond Trial Br...   \n",
       "18540  Facebook To Add 10,000 Jobs In EU For Metavers...   \n",
       "18541  Tech Crackdown Hasn't Halted Chinese Firms' Bl...   \n",
       "18542  Celo to Be Fastest EVM Chain by End of 2022, C...   \n",
       "\n",
       "                                                     url  \n",
       "0      https://cointelegraph.com/news/allbridge-to-fi...  \n",
       "1      https://cryptopotato.com/bitcoin-hodl-patterns...  \n",
       "2      https://cointelegraph.com/news/eth-hits-7-mont...  \n",
       "3      https://cointelegraph.com/news/marathon-digita...  \n",
       "4      https://cryptopotato.com/brazilian-finance-gia...  \n",
       "...                                                  ...  \n",
       "18538  https://cryptonews.com/news/cream-finance-suff...  \n",
       "18539  https://cryptonews.com/news/french-central-ban...  \n",
       "18540  https://cryptonews.com/news/facebook-to-add-10...  \n",
       "18541  https://cryptonews.com/news/tech-crackdown-has...  \n",
       "18542  https://cryptonews.com/news/celo-to-be-fastest...  \n",
       "\n",
       "[18543 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying csv (for training and testing) \n",
    "# This dataset is from https://www.kaggle.com/datasets/oliviervha/crypto-news/data\n",
    "# Note that for this dataset, Crypto news is scraped from the web. The sentiment analysis is performed using textblob.\n",
    "# Hence, sentiment analysis is not entirely correct but it's a good starting point\n",
    "dataset = pd.read_csv('cryptonews.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce2bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-05 06:52:09</td>\n",
       "      <td>Allbridge to first begin repaying stuck bridge...</td>\n",
       "      <td>{'class': 'negative', 'polarity': -0.03, 'subj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-05 06:19:00</td>\n",
       "      <td>Bitcoin Hodl Patterns Indicate Cycle Shift to ...</td>\n",
       "      <td>{'class': 'neutral', 'polarity': 0.0, 'subject...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-05 05:09:44</td>\n",
       "      <td>ETH hits 7-month high ahead of Shanghai and Ca...</td>\n",
       "      <td>{'class': 'negative', 'polarity': -0.04, 'subj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-05 01:09:52</td>\n",
       "      <td>Marathon Digital posts quarterly record of 2,1...</td>\n",
       "      <td>{'class': 'positive', 'polarity': 0.07, 'subje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-04-04 23:49:00</td>\n",
       "      <td>Brazilian Finance Giant BTG Pactual to Issue a...</td>\n",
       "      <td>{'class': 'positive', 'polarity': 0.4, 'subjec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18538</th>\n",
       "      <td>2021-10-27 15:17:00</td>\n",
       "      <td>Cream Finance Suffers Another Exploit as Attac...</td>\n",
       "      <td>{'class': 'neutral', 'polarity': 0.0, 'subject...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18539</th>\n",
       "      <td>2021-10-19 13:39:00</td>\n",
       "      <td>French Central Bank's Blockchain Bond Trial Br...</td>\n",
       "      <td>{'class': 'positive', 'polarity': 0.1, 'subjec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18540</th>\n",
       "      <td>2021-10-18 13:58:00</td>\n",
       "      <td>Facebook To Add 10,000 Jobs In EU For Metavers...</td>\n",
       "      <td>{'class': 'positive', 'polarity': 0.14, 'subje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18541</th>\n",
       "      <td>2021-10-15 00:00:00</td>\n",
       "      <td>Tech Crackdown Hasn't Halted Chinese Firms' Bl...</td>\n",
       "      <td>{'class': 'neutral', 'polarity': 0.0, 'subject...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18542</th>\n",
       "      <td>2021-10-12 20:00:00</td>\n",
       "      <td>Celo to Be Fastest EVM Chain by End of 2022, C...</td>\n",
       "      <td>{'class': 'positive', 'polarity': 0.16, 'subje...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18543 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date                                              title  \\\n",
       "0      2023-04-05 06:52:09  Allbridge to first begin repaying stuck bridge...   \n",
       "1      2023-04-05 06:19:00  Bitcoin Hodl Patterns Indicate Cycle Shift to ...   \n",
       "2      2023-04-05 05:09:44  ETH hits 7-month high ahead of Shanghai and Ca...   \n",
       "3      2023-04-05 01:09:52  Marathon Digital posts quarterly record of 2,1...   \n",
       "4      2023-04-04 23:49:00  Brazilian Finance Giant BTG Pactual to Issue a...   \n",
       "...                    ...                                                ...   \n",
       "18538  2021-10-27 15:17:00  Cream Finance Suffers Another Exploit as Attac...   \n",
       "18539  2021-10-19 13:39:00  French Central Bank's Blockchain Bond Trial Br...   \n",
       "18540  2021-10-18 13:58:00  Facebook To Add 10,000 Jobs In EU For Metavers...   \n",
       "18541  2021-10-15 00:00:00  Tech Crackdown Hasn't Halted Chinese Firms' Bl...   \n",
       "18542  2021-10-12 20:00:00  Celo to Be Fastest EVM Chain by End of 2022, C...   \n",
       "\n",
       "                                               sentiment  \n",
       "0      {'class': 'negative', 'polarity': -0.03, 'subj...  \n",
       "1      {'class': 'neutral', 'polarity': 0.0, 'subject...  \n",
       "2      {'class': 'negative', 'polarity': -0.04, 'subj...  \n",
       "3      {'class': 'positive', 'polarity': 0.07, 'subje...  \n",
       "4      {'class': 'positive', 'polarity': 0.4, 'subjec...  \n",
       "...                                                  ...  \n",
       "18538  {'class': 'neutral', 'polarity': 0.0, 'subject...  \n",
       "18539  {'class': 'positive', 'polarity': 0.1, 'subjec...  \n",
       "18540  {'class': 'positive', 'polarity': 0.14, 'subje...  \n",
       "18541  {'class': 'neutral', 'polarity': 0.0, 'subject...  \n",
       "18542  {'class': 'positive', 'polarity': 0.16, 'subje...  \n",
       "\n",
       "[18543 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only load 3 relevant columns\n",
    "# Note that the date is essential for training the model, especially when we are combining the models to boost signal generation\n",
    "dataset = dataset[['date','title', 'sentiment']]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec961c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-05 06:52:09</td>\n",
       "      <td>Allbridge to first begin repaying stuck bridge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-05 06:19:00</td>\n",
       "      <td>Bitcoin Hodl Patterns Indicate Cycle Shift to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-05 05:09:44</td>\n",
       "      <td>ETH hits 7-month high ahead of Shanghai and Ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-05 01:09:52</td>\n",
       "      <td>Marathon Digital posts quarterly record of 2,1...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-04-04 23:49:00</td>\n",
       "      <td>Brazilian Finance Giant BTG Pactual to Issue a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18538</th>\n",
       "      <td>2021-10-27 15:17:00</td>\n",
       "      <td>Cream Finance Suffers Another Exploit as Attac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18539</th>\n",
       "      <td>2021-10-19 13:39:00</td>\n",
       "      <td>French Central Bank's Blockchain Bond Trial Br...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18540</th>\n",
       "      <td>2021-10-18 13:58:00</td>\n",
       "      <td>Facebook To Add 10,000 Jobs In EU For Metavers...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18541</th>\n",
       "      <td>2021-10-15 00:00:00</td>\n",
       "      <td>Tech Crackdown Hasn't Halted Chinese Firms' Bl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18542</th>\n",
       "      <td>2021-10-12 20:00:00</td>\n",
       "      <td>Celo to Be Fastest EVM Chain by End of 2022, C...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18543 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date                                              title  \\\n",
       "0      2023-04-05 06:52:09  Allbridge to first begin repaying stuck bridge...   \n",
       "1      2023-04-05 06:19:00  Bitcoin Hodl Patterns Indicate Cycle Shift to ...   \n",
       "2      2023-04-05 05:09:44  ETH hits 7-month high ahead of Shanghai and Ca...   \n",
       "3      2023-04-05 01:09:52  Marathon Digital posts quarterly record of 2,1...   \n",
       "4      2023-04-04 23:49:00  Brazilian Finance Giant BTG Pactual to Issue a...   \n",
       "...                    ...                                                ...   \n",
       "18538  2021-10-27 15:17:00  Cream Finance Suffers Another Exploit as Attac...   \n",
       "18539  2021-10-19 13:39:00  French Central Bank's Blockchain Bond Trial Br...   \n",
       "18540  2021-10-18 13:58:00  Facebook To Add 10,000 Jobs In EU For Metavers...   \n",
       "18541  2021-10-15 00:00:00  Tech Crackdown Hasn't Halted Chinese Firms' Bl...   \n",
       "18542  2021-10-12 20:00:00  Celo to Be Fastest EVM Chain by End of 2022, C...   \n",
       "\n",
       "       sentiment  \n",
       "0              0  \n",
       "1              1  \n",
       "2              0  \n",
       "3              2  \n",
       "4              2  \n",
       "...          ...  \n",
       "18538          1  \n",
       "18539          2  \n",
       "18540          2  \n",
       "18541          1  \n",
       "18542          2  \n",
       "\n",
       "[18543 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the dataset so as to not tamper with the original csv file\n",
    "df = dataset.copy()\n",
    "\n",
    "def map_sentiment(sentiment_str: str) -> int:\n",
    "    \"\"\"\n",
    "    Parses and maps a sentiment string to an integer label.\n",
    "\n",
    "    Args:\n",
    "        sentiment_str (str): A string representation of a dictionary containing\n",
    "                             a 'class' key (e.g., \"{'class': 'Positive'}\").\n",
    "\n",
    "    Returns:\n",
    "        int: The mapped sentiment value:\n",
    "             - 0 for 'negative'\n",
    "             - 1 for 'neutral'\n",
    "             - 2 for 'positive'\n",
    "             - -1 for unknown classes or parsing errors\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert string to a Python dictionary\n",
    "        sentiment_dict = ast.literal_eval(sentiment_str)\n",
    "        # Extract the 'class' value and convert it to lowercase\n",
    "        sentiment_class = sentiment_dict['class'].lower()\n",
    "        \n",
    "        # Map the sentiment class to an integer\n",
    "        if sentiment_class == 'negative':\n",
    "            return 0\n",
    "        elif sentiment_class == 'neutral':\n",
    "            return 1\n",
    "        elif sentiment_class == 'positive':\n",
    "            return 2\n",
    "        else:\n",
    "            return -1  # for unknown classes\n",
    "    except:\n",
    "        return -1  # in case of parsing errors\n",
    "\n",
    "# Apply the mapping function to the sentiment column\n",
    "df['sentiment'] = df['sentiment'].apply(map_sentiment)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b66e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "2    8296\n",
       "1    6417\n",
       "0    3830\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of sentiments for each sentiment value\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e3ce79",
   "metadata": {},
   "source": [
    "This shows that the sentiment is not exactly balanced in the dataset. This means that when training the model, it might learn to favor positive predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9865428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans a given text string by removing special characters and converting it to lowercase.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: A cleaned version of the text with:\n",
    "             - All characters in lowercase\n",
    "             - Punctuation removed\n",
    "             - Leading and trailing whitespace stripped\n",
    "    \"\"\"\n",
    "    text = str(text).lower()  # Ensure it's a string and lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text.strip()\n",
    "\n",
    "# Apply the clean text format onto the dataset\n",
    "df['title'] = df['title'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c230394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words commonly used in market discussions that don't contribute much to sentiment\n",
    "# Ideally, we would start with standard stopword lists from libraries like to handle common English words like \"the\" and \"in.\" \n",
    "# Then, we would automatically build a domain-specific stopwords list by analyzing term frequency \n",
    "# in the dataset, identifying high-frequency terms that carry little sentiment. \n",
    "# However, due to time constraints and a lack of a proper dataset/API, we have opted for simpler approach\n",
    "general_stopwords = [\n",
    "    'price', 'market', 'today', 'time', 'day', 'week', 'year', 'bitcoin', 'ethereum', \n",
    "    'crypto', 'cryptocurrency', 'token', 'coin', 'btc', 'eth', 'xrp', 'altcoin', \n",
    "    'blockchain', 'exchange', 'investor', 'trading', 'trade', 'buy', 'sell', \n",
    "    'bullish', 'bearish', 'pump', 'dump', 'volume', 'liquidity', 'whale', 'whales',\n",
    "    'tokenomics', 'supply', 'demand', 'chart', 'technical', 'analysis', 'ta', 'fa'\n",
    "]\n",
    "neutral_crypto_terms = [\n",
    "    'defi', 'nft', 'dao', 'dapp', 'smart', 'contract', 'layer', 'l1', 'l2', \n",
    "    'staking', 'yield', 'farming', 'governance', 'validator', 'node', 'gas', 'fee',\n",
    "    'wallet', 'address', 'transaction', 'block', 'mining', 'miner', 'pow', 'pos',\n",
    "    'cex', 'dex', 'kyc', 'aml', 'airdop', 'ido', 'ieo', 'stablecoin', 'usdt', 'usdc'\n",
    "]\n",
    "financial_noise = [\n",
    "    'dollar', 'usd', 'fiat', 'capital', 'investment', 'return', 'profit', 'loss', \n",
    "    'portfolio', 'hedge', 'risk', 'volatility', 'leverage', 'margin', 'short', 'long',\n",
    "    'entry', 'exit', 'ath', 'atl', 'resistance', 'support', 'fibonacci', 'rsi', 'macd'\n",
    "]\n",
    "custom_stopwords = general_stopwords + neutral_crypto_terms + financial_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8ea8f7",
   "metadata": {},
   "source": [
    "TF-IDF Vectorization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ffd45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF Vectorizer to extract unigrams and bigrams\n",
    "# - max_features: limits vocabulary size to top 100,000 terms\n",
    "# - stop_words: use custom-defined stopwords to filter noise\n",
    "# - ngram_range: include unigrams and bigrams\n",
    "# - min_df: ignore terms that appear in fewer than 2 documents\n",
    "# - max_df: ignore terms that appear in more than 95% of documents (too common)\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=100000,               \n",
    "    stop_words= custom_stopwords,            \n",
    "    ngram_range=(1, 2),             \n",
    "    min_df=2,                       \n",
    "    max_df=0.95                     \n",
    ")\n",
    "\n",
    "# Transform the 'title' column into TF-IDF features\n",
    "X_tfidf = tfidf.fit_transform(df['title'])\n",
    "\n",
    "# Set target labels for model training\n",
    "y = df['sentiment'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a86816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.35      0.31       745\n",
      "           1       0.43      0.43      0.43      1298\n",
      "           2       0.53      0.47      0.50      1666\n",
      "\n",
      "    accuracy                           0.43      3709\n",
      "   macro avg       0.41      0.42      0.41      3709\n",
      "weighted avg       0.45      0.43      0.44      3709\n",
      "\n",
      "Accuracy: 0.4327311943920194\n"
     ]
    }
   ],
   "source": [
    "# Train-test split using TF-IDF features\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize Logistic Regression with increased max_iter to ensure convergence\n",
    "model = LogisticRegression(max_iter=10000) \n",
    "\n",
    "# Compute sample weights to handle imbalanced classes as we've observed previously that the dataset is highly imbalanced\n",
    "weights = class_weight.compute_sample_weight('balanced', y_train)\n",
    "\n",
    "# Train the model using the computed class weights\n",
    "model.fit(X_train, y_train, sample_weight = weights)\n",
    "# Predict sentiment labels on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using classification metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceb993e",
   "metadata": {},
   "source": [
    "Precision: How many of the predicted class labels were actually correct<br>\n",
    "Recall: How many actual class labels were correctly predicted<br>\n",
    "F1-score: The balance between precision and recall (good for imbalanced data)<br>\n",
    "Support: The number of actual examples for each class in the test set<br>\n",
    "\n",
    "<div>From the summary table above, we can see that the TF-IDF model performs relatively poorly. There are several reasons to this:</div>\n",
    "<div>- High Dimensionality -> Sparse Vectors. The vocalubary grows really large on a really big dataset. This makes learning difficult for this model because it's trying to find a pattern in a giant mesh of words</div>\n",
    "<div>- TF-IDF doesn't learn. It only works on word frequency and no analysis of words is done</div>\n",
    "<div>- Struggles with imbalanced data. As evident in the table analysis above, some classes have more samples than others. Even if I've specificed for the model to pay more attention to underrepresented classes, it still struggles.</div>\n",
    "    \n",
    "<br>Hence, we can conclude that this is not that great of a model when it comes to sentiment analysis for a big dataset<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5e0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save both model and vectorizer so we don't have to train the model again next time and we can reuse it to predict\n",
    "# The reason why we need to save both the model and vectorizer is because TF-IDF vectorizer transforms raw text into numerical vectors\n",
    "# Meanwhile, the model doesn’t work on raw text — it only understands numbers.\n",
    "joblib.dump(model, 'tfidf_sentiment_model.pkl')\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b56e4e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using the model, load the saved model and TF-IDF vectorizer\n",
    "model = joblib.load('tfidf_sentiment_model.pkl')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "texts = df['title']\n",
    "\n",
    "# Transform the texts using the vectorizer\n",
    "X_vectorized = vectorizer.transform(texts)\n",
    "\n",
    "# Predict using the trained model\n",
    "df['predicted_sentiment'] = model.predict(X_vectorized)\n",
    "\n",
    "# Save the predictions to a new CSV\n",
    "df.to_csv(\"tfidf_predicted_sentiment_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760ca1a6",
   "metadata": {},
   "source": [
    "Transformers (BERT model) Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a0f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-05 06:52:09</td>\n",
       "      <td>Allbridge to first begin repaying stuck bridge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-05 06:19:00</td>\n",
       "      <td>Bitcoin Hodl Patterns Indicate Cycle Shift to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-05 05:09:44</td>\n",
       "      <td>ETH hits 7-month high ahead of Shanghai and Ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-05 01:09:52</td>\n",
       "      <td>Marathon Digital posts quarterly record of 2,1...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-04-04 23:49:00</td>\n",
       "      <td>Brazilian Finance Giant BTG Pactual to Issue a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18538</th>\n",
       "      <td>2021-10-27 15:17:00</td>\n",
       "      <td>Cream Finance Suffers Another Exploit as Attac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18539</th>\n",
       "      <td>2021-10-19 13:39:00</td>\n",
       "      <td>French Central Bank's Blockchain Bond Trial Br...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18540</th>\n",
       "      <td>2021-10-18 13:58:00</td>\n",
       "      <td>Facebook To Add 10,000 Jobs In EU For Metavers...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18541</th>\n",
       "      <td>2021-10-15 00:00:00</td>\n",
       "      <td>Tech Crackdown Hasn't Halted Chinese Firms' Bl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18542</th>\n",
       "      <td>2021-10-12 20:00:00</td>\n",
       "      <td>Celo to Be Fastest EVM Chain by End of 2022, C...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18543 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date                                              title  \\\n",
       "0      2023-04-05 06:52:09  Allbridge to first begin repaying stuck bridge...   \n",
       "1      2023-04-05 06:19:00  Bitcoin Hodl Patterns Indicate Cycle Shift to ...   \n",
       "2      2023-04-05 05:09:44  ETH hits 7-month high ahead of Shanghai and Ca...   \n",
       "3      2023-04-05 01:09:52  Marathon Digital posts quarterly record of 2,1...   \n",
       "4      2023-04-04 23:49:00  Brazilian Finance Giant BTG Pactual to Issue a...   \n",
       "...                    ...                                                ...   \n",
       "18538  2021-10-27 15:17:00  Cream Finance Suffers Another Exploit as Attac...   \n",
       "18539  2021-10-19 13:39:00  French Central Bank's Blockchain Bond Trial Br...   \n",
       "18540  2021-10-18 13:58:00  Facebook To Add 10,000 Jobs In EU For Metavers...   \n",
       "18541  2021-10-15 00:00:00  Tech Crackdown Hasn't Halted Chinese Firms' Bl...   \n",
       "18542  2021-10-12 20:00:00  Celo to Be Fastest EVM Chain by End of 2022, C...   \n",
       "\n",
       "       sentiment  \n",
       "0              0  \n",
       "1              1  \n",
       "2              0  \n",
       "3              2  \n",
       "4              2  \n",
       "...          ...  \n",
       "18538          1  \n",
       "18539          2  \n",
       "18540          2  \n",
       "18541          1  \n",
       "18542          2  \n",
       "\n",
       "[18543 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = dataset.copy()\n",
    "\n",
    "# Apply the mapping function to the sentiment column\n",
    "df2['sentiment'] = df2['sentiment'].apply(map_sentiment)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea97b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14834/14834 [00:03<00:00, 4027.84 examples/s]\n",
      "Map: 100%|██████████| 3709/3709 [00:00<00:00, 4311.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Change to hugging face dataset to apply to the BERT model\n",
    "df2 = Dataset.from_pandas(df2)\n",
    "# Split the dataset into training and testing sets\n",
    "df_BERT = df2.train_test_split(test_size = 0.2)\n",
    "\n",
    "# Load the DistilBERT tokenizer (uncased version for lowercase text handling)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples: Dict[str, List[Any]]) -> Dict[str, List[Any]]:\n",
    "    \"\"\"\n",
    "    Tokenizes a batch of examples using the DistilBERT tokenizer.\n",
    "\n",
    "    This function is intended to be used with Hugging Face Datasets' `.map()` method\n",
    "    with `batched=True`. It processes a batch of text samples and prepares them\n",
    "    for input into a transformer model.\n",
    "\n",
    "    Args:\n",
    "        examples (Dict[str, List[Any]]): A dictionary where each key corresponds\n",
    "                                         to a field in the dataset. Must contain:\n",
    "                                         - \"title\": a list of tweet texts (strings)\n",
    "                                         - \"sentiment\": a list of integer labels\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[Any]]: A dictionary containing tokenized BERT inputs:\n",
    "            - \"input_ids\": list of token ID sequences (integers)\n",
    "            - \"attention_mask\": list of masks (1 for real token, 0 for padding)\n",
    "            - \"labels\": original sentiment labels copied from the input, required\n",
    "                        for supervised training (classification)\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"title\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "    # Add labels to the tokenized inputs\n",
    "    tokenized_inputs[\"labels\"] = examples[\"sentiment\"] \n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = df_BERT.map(tokenize_function, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d0147e",
   "metadata": {},
   "source": [
    "Ideally we would use ProsusAI/finbert model as it is more suitable for crypto market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c75e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load a pretrained DistilBERT model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=3,                                           # 3 classes: negative, neutral, positive\n",
    "    ignore_mismatched_sizes=True ,                          # Allows loading even if the classifier head doesn't match\n",
    "    id2label={0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    ")\n",
    "\n",
    "# Set device to GPU if available, else fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Move the model to the appropriate device for training/inference\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77a5069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyperparameters for Hugging Face Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert_finetuned\",  # Directory to save model checkpoints\n",
    "    eval_strategy=\"epoch\",                # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,                   # Learning rate for AdamW optimizer (more consistent regularization and better generalization)\n",
    "    per_device_train_batch_size=16,       # Batch size per device during training\n",
    "    per_device_eval_batch_size=16,        # Batch size per device during evaluation\n",
    "    num_train_epochs=3,                   # Number of training epochs\n",
    "    weight_decay=0.01,                    # L2 regularization strength (reduce overfitting in a machine learning model)\n",
    "    save_strategy=\"epoch\",                # Save model checkpoint at each epoch\n",
    "    load_best_model_at_end=True,          # Load best model based on eval loss\n",
    "    fp16=True,                            # Use mixed precision if NVIDIA GPU is available\n",
    "    gradient_accumulation_steps=2,        # Accumulate gradients to simulate larger batch size\n",
    "    dataloader_num_workers=4,             # Number of subprocesses for data loading (speed boost)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab4f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred: tuple) -> dict:\n",
    "    \"\"\"\n",
    "    Computes evaluation metrics: accuracy and weighted F1 score for model predictions.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (tuple): A tuple containing two elements:\n",
    "            - logits (numpy.ndarray): The raw output from the model (predicted scores).\n",
    "            - labels (numpy.ndarray): The true labels (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the following keys:\n",
    "            - 'accuracy': Accuracy score (correct predictions / total predictions)\n",
    "            - 'f1': Weighted F1 score (harmonic mean of precision and recall, weighted by class support)\n",
    "    \"\"\"\n",
    "    # Unpack the tuple into logits (model outputs) and labels (true values)\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # Convert the logits (model outputs) to predicted class labels (0, 1, 2 for negative, neutral, positive)\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Return the metrics in a dictionary\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4b8244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1392' max='1392' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1392/1392 05:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.018392</td>\n",
       "      <td>0.495282</td>\n",
       "      <td>0.427845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.029900</td>\n",
       "      <td>1.008655</td>\n",
       "      <td>0.497169</td>\n",
       "      <td>0.439211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.976200</td>\n",
       "      <td>1.018938</td>\n",
       "      <td>0.489889</td>\n",
       "      <td>0.474548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1392, training_loss=0.9812447997345322, metrics={'train_runtime': 319.9338, 'train_samples_per_second': 139.098, 'train_steps_per_second': 4.351, 'total_flos': 1473792326272512.0, 'train_loss': 0.9812447997345322, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,                                # The model to be trained (DistilBERT in this case)\n",
    "    args=training_args,                         # Hyperparameters and training configuration (learning rate, batch size, etc.)\n",
    "    train_dataset=tokenized_dataset[\"train\"],   # The training dataset (tokenized data)\n",
    "    eval_dataset=tokenized_dataset[\"test\"],     # The evaluation dataset (tokenized data)\n",
    "    compute_metrics=compute_metrics,            # The function to compute evaluation metrics (accuracy and F1 score)\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83620217",
   "metadata": {},
   "source": [
    "The reason why this model is also not providing good results, is likely due to:\n",
    "<div>- Texts too short/vague (headlines without clear sentiment cues)</div>\n",
    "<div>- DistilBERT may be too small for this task</div>\n",
    "<div>- Too much noise in the dataset</div>\n",
    "\n",
    "<br>  When training NLP models, having too much noise in the data—such as irrelevant content, typos, sarcasm, or inconsistent labeling can negatively affect the model’s ability to learn meaningful patterns. This leads to poor generalization and lower accuracy as shown above, especially in real-world applications. Denoising helps clean the data to ensure the model focuses on relevant features and learns more effectively. However, due to time constraints, we are currently unable to perform a proper denoising process. However, if we were to have a proper dataset and with proper denoising techniques, we are confident to be able to improve our signal generation.</br>\n",
    "<br> Besides, looking at Training Loss (0.97) and Validation Loss (~1.03), which are almost identical. This means the model isn't improving on the training data, and it's not generalizing to new data (validation set).</br>\n",
    "\n",
    "<br> However, on the bright side, this means that there are more room for improvement for the current NLP model</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf160691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert-tokenizer\\\\tokenizer_config.json',\n",
       " 'bert-tokenizer\\\\special_tokens_map.json',\n",
       " 'bert-tokenizer\\\\vocab.txt',\n",
       " 'bert-tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the final model\n",
    "trainer.save_model(\"bert-sentiment-model\")\n",
    "tokenizer.save_pretrained(\"bert-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d275d119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When using the model, load the saved model and tokenizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"bert-sentiment-model\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"bert-tokenizer\")\n",
    "model.eval() # This prevents the model to keep training (provides different output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
